---
title: "dbplyrでデータサイエンス100本ノック(構造化データ加工編)."
author: "Shena"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
description: "This is a minimal example of using
  the bookdown package to write a book."
github-repo: 'Shena4746/datascience-100knocks-preprocess-R'
---

```{r setup, eval = TRUE, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, warning = FALSE)
```

# 前置き

## 概要
このドキュメントは, [データサイエンス100本ノック（構造化データ加工編）](https://github.com/The-Japan-DataScientist-Society/100knocks-preprocess) を R で解いた記録. dbplyr パッケージを利用して, 手元にデータをダウンロードせず, できるだけローカルの計算資源に依存しない方針をとる. 

:::note
全問の回答は載せていない. 特に, SQL の方が楽にできる問題はスキップしている.
:::

## dbplyr とは

詳細は以下に貼る公式ドキュメントや解説記事に譲り, ここでは大雑把に書く. dbplyr とは, dplyr の文法で書いた加工処理を SQL クエリに翻訳して実行し, その結果を返してくれるパッケージ. 

- [Introduction to dbplyr](https://dbplyr.tidyverse.org/articles/dbplyr.html)
- [巨大なデータがSQLサーバーにあるときに、Rでどう立ち向かうかマニュアル：dbplyrパッケージを中心として](https://yutatoyama.github.io/note/intro_R_for_SQL.html)

手元にデータをダウンロードしようとしなければ, SQL の場合と同様に, クエリの実行コストはサーバー側が持ってくれる. dbplyr で SQL に翻訳可能なコードのみ利用可能. したがって, 手元にデータを直接ダウンロードした場合に比べて, 必然的に道具立ては少なくなる. なお, この翻訳可能性は個別のコード単位で決定されるものであり, パッケージ単位で決まっているわけではない. この翻訳問題を回避するには, `collect()` を使ってクエリ実行後の実データをダウンロードする必要があるが, それが一般に可能なのは `summarise()` などでデータサイズが十分に小さくなった場合であろう.